{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100c6adc-5afc-4450-9b47-be75d5c86684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top candidate recordings with multiple annotators and annotations:\n",
      "        filename  num_annotators  total_annotations\n",
      "3571  407115.mp3               3                 10\n",
      "2800  352781.mp3               3                  7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load annotations\n",
    "annotations_df = pd.read_csv(\"annotations.csv\")\n",
    "\n",
    "# Group by filename and annotator to count annotations per annotator per file\n",
    "grouped = annotations_df.groupby(['filename', 'annotator']).size().reset_index(name='num_annotations')\n",
    "\n",
    "# Now count how many unique annotators per file\n",
    "annotator_counts = grouped.groupby('filename')['annotator'].nunique().reset_index(name='num_annotators')\n",
    "\n",
    "# Merge back to get total annotations per file\n",
    "annotation_counts = annotations_df.groupby('filename').size().reset_index(name='total_annotations')\n",
    "merged = pd.merge(annotator_counts, annotation_counts, on='filename')\n",
    "\n",
    "# Filter files with at least 2 annotators and multiple annotations\n",
    "candidate_files = merged[(merged['num_annotators'] >= 2) & (merged['total_annotations'] > 2)]\n",
    "\n",
    "# Show top 2 files\n",
    "top_candidates = candidate_files.sort_values(by='num_annotators', ascending=False).head(2)\n",
    "print(\"Top candidate recordings with multiple annotators and annotations:\")\n",
    "print(top_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9606f55a-8088-478b-a31c-7360d8a8cdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Annotations for 407115.mp3 =====\n",
      "\n",
      "-- Annotator: 102834162874743039972126867430893976507845628340201871147467671756946315453713 --\n",
      "    onset    offset                                  text\n",
      " 0.000000 29.087982 The sound of a lively crowd outdoors.\n",
      "16.856425 17.404106               A muffled tonal signal.\n",
      "\n",
      "-- Annotator: 62883765306800336674960299070997168485387625840275185443012937071154406071377 --\n",
      "    onset    offset                                                    text\n",
      " 0.037776 29.030748                        people talking in the background\n",
      " 0.094440 29.030748       continuous sewing machine noise in the background\n",
      "16.885809 18.132413                  horn honking once at a moderate volume\n",
      "21.532240 22.401085 wheel scratching slightly sharp noise in the background\n",
      "\n",
      "-- Annotator: 84704702810492203570508455757276662194775685610231578309011132791728316253685 --\n",
      "    onset    offset                                             text\n",
      " 0.000000 22.525530 the sound of somebody sweeping in the background\n",
      " 0.000000 29.030748                 people talking in the background\n",
      "16.829419 17.476705                                  a honking sound\n",
      "21.457510 22.266616                                  a beeping sound\n",
      "\n",
      "===== Annotations for 352781.mp3 =====\n",
      "\n",
      "-- Annotator: 411636573135802475484269253664194870658670826351810573916181360144468290346 --\n",
      " onset    offset                                                    text\n",
      "   0.0 16.451995 Muffled voices of people having a conversation on a bus\n",
      "   0.0 16.451995     A low hum of a bus driving steadily across the road\n",
      "\n",
      "-- Annotator: 57835086208742693355557312559196303821474911200770380653032223797734258614886 --\n",
      "   onset    offset                                                                           text\n",
      "0.028448 16.400499 White noise is heard in the close space, a constant, soft hiss filling the air\n",
      "0.056897 16.400499                   People are talking nearby, their voices unclear and muddled.\n",
      "\n",
      "-- Annotator: 94057971961546359846229431462387685271883384943446523429356802373347207481651 --\n",
      "   onset    offset                                                               text\n",
      "0.000000 16.400499 An engine of a heavy vehicle is running silently in the background\n",
      "0.018819 16.400499                   Multiple people speaking close by evenly indoors\n",
      "1.097183 16.400499                        A radio is silently playing in the distance\n"
     ]
    }
   ],
   "source": [
    "# Filter annotations for the two selected files\n",
    "selected_files = [\"407115.mp3\", \"352781.mp3\"]\n",
    "selected_annotations = annotations_df[annotations_df[\"filename\"].isin(selected_files)]\n",
    "\n",
    "# Sort for readability\n",
    "selected_annotations = selected_annotations.sort_values(by=[\"filename\", \"annotator\", \"onset\"])\n",
    "\n",
    "# Display the annotations grouped by file and annotator\n",
    "for filename in selected_files:\n",
    "    print(f\"\\n===== Annotations for {filename} =====\")\n",
    "    file_anns = selected_annotations[selected_annotations[\"filename\"] == filename]\n",
    "    for annotator in file_anns[\"annotator\"].unique():\n",
    "        print(f\"\\n-- Annotator: {annotator} --\")\n",
    "        ann = file_anns[file_anns[\"annotator\"] == annotator][[\"onset\", \"offset\", \"text\"]]\n",
    "        print(ann.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60da0a-a6e4-4666-94e8-cee86985850d",
   "metadata": {},
   "source": [
    "**(a) Temporal & Textual Annotation Comparison**\n",
    "\n",
    "**Audio 1 (407115.mp3)**:\n",
    "\n",
    "**Temporal Overlap**:\n",
    "\n",
    "All annotators agree on a long segment (0–29s) containing multiple events.\n",
    "\n",
    "Annotators 2 and 3 both highlight sound events between ~16s and 22s (honking, beeping, scratching).\n",
    "\n",
    "Differences in segment precision — Annotator 1 uses broader intervals, while Annotators 2 & 3 mark finer-grained events.\n",
    "\n",
    "**Textual Similarity & Differences**:\n",
    "\n",
    "All three detect people talking — described as “lively crowd,” “people talking,” and “somebody sweeping” (possibly mistaken interpretation).\n",
    "\n",
    "Annotators 2 & 3 label overlapping sound events like honks or beeps, but use slightly different language (\"horn honking\", \"a honking sound\", \"a beeping sound\").\n",
    "\n",
    "Annotator 2 uniquely identifies a sewing machine and wheel scratching, indicating higher event resolution or different interpretation.\n",
    "\n",
    "**Audio 2 (352781.mp3)**:\n",
    "\n",
    "**Temporal Annotations (Onset/Offset)**\n",
    "\n",
    "High agreement across annotators on the full duration of the audio (approx. 0 to 16.4s).\n",
    "\n",
    "Small differences in onset (e.g., 0.0 vs. 0.028 or 0.056) are minimal and likely due to human precision.\n",
    "\n",
    "All annotators treat the clip as a continuous ambient scene rather than short, isolated events.\n",
    "\n",
    "**Similarities**:\n",
    "\n",
    "All three annotators identify:\n",
    "\n",
    "Human speech: Described as \"muffled\", \"unclear\", \"multiple people\"\n",
    "\n",
    "Background noise: Captured as \"white noise\", \"engine\", \"hum\"\n",
    "\n",
    "All are describing a confined indoor-like sound environment (e.g., inside a bus or a room)\n",
    "\n",
    "**Differences**:\n",
    "\n",
    "A1 explicitly identifies the context (\"on a bus\")\n",
    "\n",
    "A2 generalizes to \"close space\" and \"white noise\"\n",
    "\n",
    "A3 adds more detail, identifying a radio and engine with specific qualifiers (\"silently\", \"evenly indoors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af67b43a-99ba-4e5d-aa06-11de20926749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords for 407115.mp3: Uganda, people, sewing, Africa, textile, mall, machines, corridor, shops, market\n",
      "Keywords for 352781.mp3: engine, field-recording, people, bus, voices, talking, radio, vehicle, ambience\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(\"metadata.csv\")\n",
    "\n",
    "# Show full column content (for keywords)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "print(\"Keywords for 407115.mp3:\", metadata_df.loc[metadata_df[\"filename\"] == \"407115.mp3\", \"keywords\"].values[0])\n",
    "print(\"Keywords for 352781.mp3:\", metadata_df.loc[metadata_df[\"filename\"] == \"352781.mp3\", \"keywords\"].values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f1a25-2798-4feb-89fc-49b2d95d70c4",
   "metadata": {},
   "source": [
    "**b)**\n",
    "\n",
    "**407115.mp3**\n",
    "\n",
    "**Matches / Reliance**\n",
    ":\n",
    "\"people talking\", \"lively crowd\" → matches people, market, mall\n",
    "\n",
    "\"sewing machine noise\" → directly reflects sewing, machines, textile\n",
    "\n",
    "**Deviations / Additions**:\n",
    "\n",
    "\"horn honking\", \"beeping\", \"wheel scratching\" → Not present in keywords\n",
    "\n",
    "These are foreground events perceived by annotators but not described in metadata\n",
    "\n",
    "**Conclusion**:\n",
    "\n",
    "Annotations rely heavily on keywords for background context (people, sewing, market scene)\n",
    "\n",
    "Annotators go beyond the metadata by identifying short, foreground sound events (e.g., honks)\n",
    "\n",
    "**352781.mp3**\n",
    "\n",
    "**Matches / Reliance**:\n",
    "\n",
    "\"engine of a heavy vehicle\", \"bus driving\" → matches engine, bus, vehicle\n",
    "\n",
    "\"people speaking\", \"voices\", \"talking\" → matches people, voices, talking\n",
    "\n",
    "\"radio playing\" → directly matches radio\n",
    "\n",
    "\"white noise\", \"ambience\" → aligns with ambience\n",
    "\n",
    "**Deviations / Additions**:\n",
    "\n",
    "Very few! Maybe \"white noise\" adds a perceptual detail, but it’s covered by ambience\n",
    "\n",
    "**Conclusion**:\n",
    "\n",
    "This is a textbook example of alignment\n",
    "\n",
    "Annotations match nearly all metadata keywords, showing strong reliance and task alignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe3f61-9f1c-465c-883c-03b1a17b6c06",
   "metadata": {},
   "source": [
    "**c)**\n",
    "\n",
    "For 407115.mp3 (Urban Environment with Crowd and Sewing Sounds)\n",
    "\n",
    "Temporal Annotation Compliance:\n",
    "\n",
    "Onset and Offset: The annotators followed the rule of annotating distinct sounds. For example:\n",
    "\n",
    "People talking: Different annotators split the crowd noise into multiple events (e.g., a continuous sound, then specific people talking). The annotations' onset/offset reflects this continuous nature of the sound, as suggested by the task (e.g., \"people talking\" marked over a longer period).\n",
    "\n",
    "Sewing machine: This was marked as a continuous sound with an onset at the start and an offset at the end of the sound.\n",
    "\n",
    "Honking, beeping, and wheel scratching: These are separate events, as per the task's rule of separating events when noticeable pauses are present. Annotators made distinct annotations for each.\n",
    "\n",
    "The temporal boundaries are consistent and correct based on the task's example. For instance, the “sewing machine noise” annotation lasts throughout the segment where the machine sound is heard, fitting the continuous sound rule.\n",
    "\n",
    "Textual Annotation Compliance:\n",
    "\n",
    "Specific Descriptions: The descriptions follow the task’s required level of detail:\n",
    "\n",
    "Source and Action: Each annotation specifies the source of the sound (e.g., \"sewing machine noise,\" \"people talking\").\n",
    "\n",
    "Descriptor: Descriptions like \"lively,\" \"muffled,\" and \"tonal\" provide insight into how the sounds were perceived.\n",
    "\n",
    "Context: Annotators provide an environmental context where relevant (e.g., “crowd outdoors,” “in the background,” “indoors”).\n",
    "\n",
    "The annotations adhere to the one description for one sound rule (e.g., separate descriptions for \"honking,\" \"wheel scratching,\" and \"people talking\").\n",
    "\n",
    "**Conclusion**:\n",
    "\n",
    "The temporal and textual annotations are fully compliant with the task description. The annotators correctly followed guidelines for separating sounds, providing detailed annotations, and aligning their work with the task’s structure.\n",
    "\n",
    "For 352781.mp3 (Bus Environment with People Talking and Engine Sounds)\n",
    "\n",
    "Temporal Annotation Compliance:\n",
    "\n",
    "Onset and Offset: The annotators marked the entire recording as a single region for continuous sounds like engine hum and ambient voices, which aligns with the continuous sound rule (e.g., \"low hum of a bus driving steadily\").\n",
    "\n",
    "In contrast, short events like horn honking or radio playing were marked separately, showing the annotators correctly handled short, separate sound events and gaps in between.\n",
    "\n",
    "The temporal boundaries for each sound event are appropriate, and the overlapping sounds (e.g., bus engine and talking) are managed correctly.\n",
    "\n",
    "Textual Annotation Compliance:\n",
    "\n",
    "Specific Descriptions: The descriptions follow the task’s rule of detailed annotations:\n",
    "\n",
    "Source and Action: Each annotation specifies the source of the sound (e.g., \"engine of a heavy vehicle,\" \"voices of people\").\n",
    "\n",
    "Descriptor: Descriptions like “hissing,” “soft,” and “muffled” describe how the sound was perceived.\n",
    "\n",
    "Context: The context is well-defined, with mentions of \"indoors,\" \"in the distance,\" and \"close by.\"\n",
    "\n",
    "No multiple events are described in a single annotation, meaning each description corresponds to one sound event.\n",
    "\n",
    "**Conclusion**:\n",
    "\n",
    "The temporal and textual annotations for this recording are also fully compliant with the task description. Annotators successfully separated distinct sounds, followed the rules for continuous versus discrete events, and provided detailed textual descriptions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
