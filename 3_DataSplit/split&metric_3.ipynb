{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c915fc0-a39f-46a8-a55e-22107e8f3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1708f706-1757-43b1-84f6-dd93828e55bd",
   "metadata": {},
   "source": [
    "# Dataset Function:\n",
    "\n",
    "### 🎧 AudioFrameDataset Class\n",
    "\n",
    "This custom PyTorch `Dataset` is designed to load **time-aligned audio features and frame-level multi-label annotations** for training sound event detection models.\n",
    "\n",
    "#### 🗂 What It Does:\n",
    "- Loads `.npz` files containing various audio features and concatenates them across time.\n",
    "- Loads corresponding frame-wise class labels for multiple sound event types.\n",
    "- Supports **multi-label classification** (more than one event per time step).\n",
    "- Optionally **binarizes labels** by averaging across multiple annotators.\n",
    "- Stores the sequence lengths up front to support **bucketing or sorting** based on time length.\n",
    "\n",
    "#### 🔍 Inputs:\n",
    "- `feature_dir`: Path to the directory with audio features.\n",
    "- `label_dir`: Path to the directory with annotation labels.\n",
    "- `file_list`: List of filenames (without extensions) to be used from the dataset.\n",
    "- `class_names`: List of all possible class labels.\n",
    "- `binary_labels`: Whether to convert annotator labels into binary labels (True by default).\n",
    "\n",
    "#### 📥 Output Per Sample:\n",
    "- `feature`: Tensor of shape `(time_steps, total_features)` — stacked features across time.\n",
    "- `multi_label`: Tensor of shape `(time_steps, num_classes)` — frame-wise binary labels for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8805fe6d-415a-4c93-bb08-cedd5e4145f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFrameDataset(Dataset):\n",
    "    def __init__(self, feature_dir, label_dir, file_list, class_names, binary_labels=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_dir (str): Directory containing audio feature .npz files\n",
    "            label_dir (str): Directory containing label .npz files\n",
    "            file_list (List[str]): List of base filenames (without extensions)\n",
    "            class_names (List[str]): List of all possible class names (e.g., [\"Alarm\", \"BirdChirp\", \"BackgroundNoise\"])\n",
    "            binary_labels (bool): If True, create binary labels for multi-label classification\n",
    "        \"\"\"\n",
    "        self.feature_dir = feature_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.file_list = file_list\n",
    "        self.class_names = class_names  # List of all class names for multi-label\n",
    "        self.binary_labels = binary_labels\n",
    "        self.lengths = []\n",
    "        for filename in self.file_list:\n",
    "            path = os.path.join(self.feature_dir, f\"{filename}.npz\")\n",
    "            features = np.load(path)['embeddings']  # Use any one modality to get time length\n",
    "            self.lengths.append(features.shape[0])\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "\n",
    "    def get_length(self, idx):\n",
    "        return self.lengths[idx]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.file_list[idx]\n",
    "\n",
    "        # Load features\n",
    "        feat_path = os.path.join(self.feature_dir, f\"{filename}.npz\")\n",
    "        features_npz = np.load(feat_path)\n",
    "\n",
    "        # Concatenate all feature arrays (time-aligned)\n",
    "        feature = np.concatenate([\n",
    "            features_npz['embeddings'],\n",
    "            features_npz['melspectrogram'],\n",
    "            features_npz['mfcc'],\n",
    "            features_npz['mfcc_delta'],\n",
    "            features_npz['mfcc_delta2'],\n",
    "            features_npz['flatness'],\n",
    "            features_npz['centroid'],\n",
    "            features_npz['flux'],\n",
    "            features_npz['energy'],\n",
    "            features_npz['power'],\n",
    "            features_npz['bandwidth'],\n",
    "            features_npz['contrast'],\n",
    "            features_npz['zerocrossingrate']\n",
    "        ], axis=-1)  # shape: (time, total_features)\n",
    "\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, f\"{filename}_labels.npz\")\n",
    "        label_npz = np.load(label_path)\n",
    "\n",
    "        # Initialize the multi-label array (binary matrix)\n",
    "        multi_label = np.zeros((feature.shape[0], len(self.class_names)))  # shape: (time, num_classes)\n",
    "\n",
    "        # Loop through each class and assign the binary values for each time step\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            if class_name in label_npz:\n",
    "                class_labels = label_npz[class_name]  # shape: (time, annotators)\n",
    "                \n",
    "                if self.binary_labels:\n",
    "                    # If binary, average across annotators and threshold\n",
    "                    multi_label[:, i] = (class_labels.mean(axis=-1) > 0.5).astype(np.float32)\n",
    "                else:\n",
    "                    # Keep the full annotator labels (optional)\n",
    "                    multi_label[:, i] = class_labels.mean(axis=-1).astype(np.float32)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        feature = torch.from_numpy(feature).float()\n",
    "        multi_label = torch.from_numpy(multi_label).float()  # Shape: (time, num_classes)\n",
    "\n",
    "        return feature, multi_label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b16b40f-0504-47ab-bc7a-6a63926d2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"labels\", \"14_labels.npz\")\n",
    "labels = np.load(filename)\n",
    "\n",
    "class_names = list(labels.keys())\n",
    "\n",
    "directory_path = \"audio_features\"\n",
    "\n",
    "file_list = [os.path.splitext(f)[0] for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f)) and f.endswith('.npz')]\n",
    "\n",
    "dataset = AudioFrameDataset(\"audio_features\", \"labels\", file_list, class_names)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a35b8-45fe-42b3-b2f3-724a526f1b48",
   "metadata": {},
   "source": [
    "### 🧩 Collate Function for Padding\n",
    "\n",
    "When working with variable-length sequences, we use a custom **collate function** to pad each batch to the same length. This ensures tensors can be stacked and passed through the model in batches.\n",
    "\n",
    "#### 🔶 Input (X):\n",
    "- **Shape:** `(time_steps, features)`\n",
    "- **Padding:** Extra time steps are added with **all features set to zero**.\n",
    "- **Interpretation:** In the feature space, these padded regions contain no meaningful data. The model may learn to treat them as *silence* or simply ignore them.\n",
    "\n",
    "#### 🔷 Labels (Y):\n",
    "- **Shape:** `(time_steps, num_classes)` — one-hot encoded label vectors\n",
    "- **Padding:** Extra time steps are added with **all class values set to zero**.\n",
    "- **Interpretation:** These indicate *unlabeled* time regions and **should be masked** during training to avoid misleading the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f37d59e6-33a3-4ab4-b27a-d5254f319ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Sort by the length of the feature (time steps)\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)  # Sort by the length of the feature tensor\n",
    "    \n",
    "    padded_features = []\n",
    "    padded_labels = []\n",
    "    \n",
    "    max_len_feature = max([len(features) for features, _ in batch])  # Maximum sequence length (time steps) for features\n",
    "    max_len_label = max([len(labels) for _, labels in batch])  # Maximum sequence length (time steps) for labels\n",
    "    \n",
    "    for features, labels in batch:\n",
    "        feature_len = len(features)\n",
    "        label_len = len(labels)\n",
    "        \n",
    "        # Padding features\n",
    "        if feature_len < max_len_feature:\n",
    "            pad_size = max_len_feature - feature_len\n",
    "            features = torch.cat([features, torch.zeros(pad_size, features.size(1))], dim=0)  # Pad features\n",
    "        \n",
    "        # Padding labels\n",
    "        if label_len < max_len_label:\n",
    "            pad_size = max_len_label - label_len\n",
    "            labels = torch.cat([labels, torch.zeros(pad_size, labels.size(1))], dim=0)  # Pad labels\n",
    "        \n",
    "        padded_features.append(features)\n",
    "        padded_labels.append(labels)\n",
    "\n",
    "    # Stack padded features and labels into tensors\n",
    "    padded_features = torch.stack(padded_features)\n",
    "    padded_labels = torch.stack(padded_labels)\n",
    "    \n",
    "    return padded_features, padded_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ae4f4-efc9-44fd-b193-552d9227a046",
   "metadata": {},
   "source": [
    "### 🎯 Notes on Accuracy and Training\n",
    "\n",
    "- Padding `X` with zeros is **not just \"silence\"** in the audio sense — it represents **a lack of information** in feature space. The model should learn not to associate this with any real class.\n",
    "\n",
    "- Padding `Y` with zeros **must be handled with a mask**. This mask is applied during loss and accuracy computation to ensure:\n",
    "  - Padded labels don't contribute to the loss.\n",
    "  - The model isn't penalized for predictions on time steps with no true labels.\n",
    "\n",
    "✅ This setup allows efficient batching without compromising model accuracy or training signal quality.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415401aa-698e-4be2-9c3e-2904cdc1739f",
   "metadata": {},
   "source": [
    "### 🪣 Bucket Sampler Function:\n",
    "\n",
    "#### ⚙️ How It Works:\n",
    "- Sorts or groups samples by their sequence length (e.g., number of time steps).\n",
    "- Forms batches where the sequence lengths within a batch are as similar as possible.\n",
    "- Optionally shuffles batches while maintaining internal length consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc87502-9f20-4872-b366-bd3d958cb950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "\n",
    "class BucketBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Sort indices by length\n",
    "        self.sorted_indices = sorted(range(len(dataset)), key=lambda i: dataset.get_length(i))\n",
    "\n",
    "        # Divide into buckets\n",
    "        self.batches = [\n",
    "            self.sorted_indices[i:i + batch_size]\n",
    "            for i in range(0, len(self.sorted_indices), batch_size)\n",
    "        ]\n",
    "\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbaecf-84b9-4bb6-a2ce-e96b171427f0",
   "metadata": {},
   "source": [
    "# Splitting the Data:\n",
    "\n",
    "#### 🔀 Function: `create_train_val_test_splits`\n",
    "This function splits a list of audio feature filenames into:\n",
    "- **Train set**  \n",
    "- **Validation set**  \n",
    "- **Test set**\n",
    "\n",
    "It ensures reproducibility via a fixed `random_state`, and properly adjusts the sizes of the validation and test splits.\n",
    "\n",
    "#### 🧪 Dataset Initialization\n",
    "Using the file lists returned by the split function, we create separate instances of `AudioFrameDataset` for:\n",
    "- `train_dataset`\n",
    "- `val_dataset`\n",
    "- `test_dataset`\n",
    "\n",
    "Each dataset points to its respective `.npz` files for features and labels.\n",
    "\n",
    "#### 🚚 Efficient Loading with Bucketing\n",
    "To improve batching efficiency for variable-length sequences:\n",
    "- We use a custom `BucketBatchSampler` that groups sequences of similar lengths into the same batch (reduces padding).\n",
    "- Each DataLoader is created using the corresponding bucketed sampler and a `collate_fn` that handles padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c22ca12-6ff3-4a88-b2bc-cf7df1eeda58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split your dataset into train, validation, and test sets\n",
    "def create_train_val_test_splits(file_list, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    # Split into train + temp (for validation + test)\n",
    "    train_files, temp_files = train_test_split(file_list, test_size=test_size + val_size, random_state=random_state)\n",
    "    \n",
    "    # Split temp into validation and test\n",
    "    val_size_adjusted = val_size / (test_size + val_size)  # Adjust for the size of the temp set\n",
    "    val_files, test_files = train_test_split(temp_files, test_size=val_size_adjusted, random_state=random_state)\n",
    "    \n",
    "    return train_files, val_files, test_files\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"audio_features\"\n",
    "file_list = [os.path.splitext(f)[0] for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f)) and f.endswith('.npz')]\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_files, val_files, test_files = create_train_val_test_splits(file_list)\n",
    "\n",
    "# Now create instances of the AudioFrameDataset for each split\n",
    "train_dataset = AudioFrameDataset(\n",
    "    feature_dir=\"audio_features\",\n",
    "    label_dir=\"labels\",\n",
    "    file_list=train_files,\n",
    "    class_names=class_names,\n",
    "    binary_labels=True\n",
    ")\n",
    "\n",
    "val_dataset = AudioFrameDataset(\n",
    "    feature_dir=\"audio_features\",\n",
    "    label_dir=\"labels\",\n",
    "    file_list=val_files,\n",
    "    class_names=class_names,\n",
    "    binary_labels=True\n",
    ")\n",
    "\n",
    "test_dataset = AudioFrameDataset(\n",
    "    feature_dir=\"audio_features\",\n",
    "    label_dir=\"labels\",\n",
    "    file_list=test_files,\n",
    "    class_names=class_names,\n",
    "    binary_labels=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "batch_size = 32\n",
    "train_sampler = BucketBatchSampler(train_dataset, batch_size)\n",
    "val_sampler = BucketBatchSampler(val_dataset, batch_size, shuffle=False)\n",
    "test_sampler = BucketBatchSampler(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_sampler=val_sampler, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_sampler=test_sampler, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3adda996-9dda-4e54-9b61-b965665b65d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loader Info:\n",
      "Batch 1:\n",
      "  Features shape: torch.Size([32, 249, 942])\n",
      "  Labels shape: torch.Size([32, 249, 58])\n",
      "Batch 2:\n",
      "  Features shape: torch.Size([32, 210, 942])\n",
      "  Labels shape: torch.Size([32, 210, 58])\n",
      "Batch 3:\n",
      "  Features shape: torch.Size([32, 179, 942])\n",
      "  Labels shape: torch.Size([32, 179, 58])\n",
      "\n",
      "Validation Loader Info:\n",
      "Batch 1:\n",
      "  Features shape: torch.Size([32, 127, 942])\n",
      "  Labels shape: torch.Size([32, 127, 58])\n",
      "Batch 2:\n",
      "  Features shape: torch.Size([32, 130, 942])\n",
      "  Labels shape: torch.Size([32, 130, 58])\n",
      "Batch 3:\n",
      "  Features shape: torch.Size([32, 132, 942])\n",
      "  Labels shape: torch.Size([32, 132, 58])\n",
      "\n",
      "Test Loader Info:\n",
      "Batch 1:\n",
      "  Features shape: torch.Size([32, 129, 942])\n",
      "  Labels shape: torch.Size([32, 129, 58])\n",
      "Batch 2:\n",
      "  Features shape: torch.Size([32, 133, 942])\n",
      "  Labels shape: torch.Size([32, 133, 58])\n",
      "Batch 3:\n",
      "  Features shape: torch.Size([32, 137, 942])\n",
      "  Labels shape: torch.Size([32, 137, 58])\n"
     ]
    }
   ],
   "source": [
    "# Function to print a few batches from a DataLoader\n",
    "def print_loader_info(loader, loader_name):\n",
    "    print(f\"\\n{loader_name} Loader Info:\")\n",
    "    for i, (features, labels) in enumerate(loader):\n",
    "        print(f\"Batch {i+1}:\")\n",
    "        print(f\"  Features shape: {features.shape}\")  # Should be (batch_size, time_steps, num_features)\n",
    "        print(f\"  Labels shape: {labels.shape}\")      # Should be (batch_size, time_steps)\n",
    "        \n",
    "        if i == 2:  # Show only the first 3 batches to avoid too much output\n",
    "            break\n",
    "\n",
    "# Print info for each loader\n",
    "print_loader_info(train_loader, \"Training\")\n",
    "print_loader_info(val_loader, \"Validation\")\n",
    "print_loader_info(test_loader, \"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a953bc-4f15-432a-b9d6-56cf88692b01",
   "metadata": {},
   "source": [
    "# Evaluation Metric :\n",
    "\n",
    "### 📏 Balanced Accuracy (with Masking)\n",
    "\n",
    "When dealing with padded sequences (e.g., audio or time-series data of variable lengths), it's important to avoid letting the model learn from or be evaluated on artificial padding.\n",
    "\n",
    "#### 🛑 Why Masking Is Necessary:\n",
    "- **Padded values are not real data** — they're just placeholders to align sequence lengths within a batch.\n",
    "- **Including padded time steps in loss or accuracy** calculations would distort training and mislead evaluation.\n",
    "- **Masking ensures** the model focuses only on valid, meaningful data points.\n",
    "\n",
    "🧠 We use a **mask** to tell the model:  \n",
    "> “Ignore these positions — they don't carry useful information.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7efe2d-b2a0-4881-ba82-29c0bdbb4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_loss_and_accuracy(logits, labels, padding_mask, loss_fn):\n",
    "    # logits: (batch, time, num_classes)\n",
    "    # labels: (batch, time, num_classes) — one-hot\n",
    "    # padding_mask: (batch, time) — 1 if valid, 0 if padded\n",
    "\n",
    "    # Convert one-hot to class indices\n",
    "    target = labels.argmax(dim=-1)  # (batch, time)\n",
    "    pred = logits.argmax(dim=-1)    # (batch, time)\n",
    "\n",
    "    # Flatten everything\n",
    "    flat_logits = logits.view(-1, logits.shape[-1])         # (batch*time, num_classes)\n",
    "    flat_target = target.view(-1)                           # (batch*time,)\n",
    "    flat_mask = padding_mask.view(-1).bool()                # (batch*time,)\n",
    "\n",
    "    # Compute loss only on valid (unpadded) positions\n",
    "    loss_per_timestep = loss_fn(flat_logits, flat_target)   # (batch*time,)\n",
    "    masked_loss = loss_per_timestep[flat_mask].mean()\n",
    "\n",
    "    # Compute balanced accuracy\n",
    "    true = flat_target[flat_mask].cpu().numpy()\n",
    "    pred = pred.view(-1)[flat_mask].cpu().numpy()\n",
    "    acc = balanced_accuracy_score(true, pred)\n",
    "\n",
    "    return masked_loss, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622fb1a-4482-4e9f-94d2-d0f07d26d08d",
   "metadata": {},
   "source": [
    "# Baseline Model:\n",
    "\n",
    "### 🧠 SimpleTimeStepClassifier\n",
    "\n",
    "This is a basic fully-connected (feedforward) neural network for classifying each time step in a sequence (e.g., audio frames or time-series data).\n",
    "\n",
    "#### 🔧 Model Architecture:\n",
    "- **Input shape:** `(batch_size, time_steps, input_dim)`\n",
    "- **Layers:**\n",
    "  - Linear layer mapping `input_dim → hidden_dim`\n",
    "  - ReLU activation\n",
    "  - Linear layer mapping `hidden_dim → num_classes`\n",
    "- The model processes each time step independently (no temporal modeling like RNNs or Transformers).\n",
    "\n",
    "#### 📤 Output:\n",
    "- Returns logits of shape `(batch_size, time_steps, num_classes)`, where each time step is classified independently.\n",
    "\n",
    "This type of model is suitable for tasks like **frame-wise classification** in audio or video, where each time step (or frame) is labeled separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5335105-ed9b-4185-872f-db281fd88ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTimeStepClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=942, hidden_dim=256, num_classes=58):\n",
    "        super(SimpleTimeStepClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, time_steps, input_dim)\n",
    "        batch_size, time_steps, _ = x.shape\n",
    "        x = x.view(-1, x.shape[-1])  # Flatten to (batch_size * time_steps, input_dim)\n",
    "        out = self.net(x)\n",
    "        return out.view(batch_size, time_steps, -1)  # Reshape back to (batch, time_steps, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "773eb46f-5fab-4b8e-aeee-8c0f9b237006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0, 0\n",
    "\n",
    "    for features, labels in dataloader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        padding_mask = (labels.sum(dim=-1) != 0).float()  # (batch, time)\n",
    "\n",
    "        logits = model(features)\n",
    "\n",
    "        loss, acc = get_masked_loss_and_accuracy(logits, labels, padding_mask, loss_fn)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in dataloader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            padding_mask = (labels.sum(dim=-1) != 0).float()\n",
    "\n",
    "            logits = model(features)\n",
    "            loss, acc = get_masked_loss_and_accuracy(logits, labels, padding_mask, loss_fn)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29eaed9d-33e2-4592-b125-9f88ae7622c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 8.0869, Balanced Accuracy: 0.1788\n",
      "  Val   Loss: 3.7401, Balanced Accuracy: 0.2245\n",
      "Epoch 2:\n",
      "  Train Loss: 3.1176, Balanced Accuracy: 0.3531\n",
      "  Val   Loss: 2.7914, Balanced Accuracy: 0.3742\n",
      "Epoch 3:\n",
      "  Train Loss: 2.4368, Balanced Accuracy: 0.4119\n",
      "  Val   Loss: 2.1601, Balanced Accuracy: 0.4333\n",
      "Epoch 4:\n",
      "  Train Loss: 2.0169, Balanced Accuracy: 0.4577\n",
      "  Val   Loss: 1.7911, Balanced Accuracy: 0.4758\n",
      "Epoch 5:\n",
      "  Train Loss: 1.8703, Balanced Accuracy: 0.4785\n",
      "  Val   Loss: 1.7801, Balanced Accuracy: 0.4756\n",
      "Epoch 6:\n",
      "  Train Loss: 1.6639, Balanced Accuracy: 0.5029\n",
      "  Val   Loss: 1.6402, Balanced Accuracy: 0.5036\n",
      "Epoch 7:\n",
      "  Train Loss: 1.5915, Balanced Accuracy: 0.5161\n",
      "  Val   Loss: 1.5460, Balanced Accuracy: 0.5159\n",
      "Epoch 8:\n",
      "  Train Loss: 1.4999, Balanced Accuracy: 0.5354\n",
      "  Val   Loss: 1.4857, Balanced Accuracy: 0.5294\n",
      "Epoch 9:\n",
      "  Train Loss: 1.4870, Balanced Accuracy: 0.5361\n",
      "  Val   Loss: 1.5400, Balanced Accuracy: 0.5267\n",
      "Epoch 10:\n",
      "  Train Loss: 1.3843, Balanced Accuracy: 0.5538\n",
      "  Val   Loss: 1.3872, Balanced Accuracy: 0.5570\n",
      "\n",
      "  Test  Loss: 1.3361, Balanced Accuracy: 0.5788\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleTimeStepClassifier().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Balanced Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f}, Balanced Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, loss_fn, device)\n",
    "print(f\"\\n  Test  Loss: {test_loss:.4f}, Balanced Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9445ed-fa93-4c0d-8673-70395e5c65f2",
   "metadata": {},
   "source": [
    " #### BACC = 57.88% pretty good? Random guessing would yield 1.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e67dd-cf03-40b1-b41c-f125d78dec3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
