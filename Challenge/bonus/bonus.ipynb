{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ee080c-9031-4539-a2d0-c1bbb941aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d7f56e-3d2b-4076-9ecc-03c4bcf4c21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/TG/anaconda3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from portable_m2d import PortableM2D as M2D\n",
    "from transformer_wrapper import BaseModelWrapper\n",
    "\n",
    "\n",
    "class M2DWrapper(BaseModelWrapper):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.m2d = M2D()\n",
    "\n",
    "    def mel_forward(self, x):\n",
    "        return self.m2d.to_normalized_feature(x)\n",
    "\n",
    "    def forward(self, spec):\n",
    "        return self.m2d.forward_mel(spec)\n",
    "\n",
    "    def separate_params(self):\n",
    "        pt_params = [[], [], [], [], [], [], [], [], [], [], [], []]\n",
    "        for k, p in self.named_parameters():\n",
    "            if any(['cls_token' in k,\n",
    "                    'pos_embed' in k,\n",
    "                    'norm_stats' in k,\n",
    "                    'patch_embed' in k]):\n",
    "                pt_params[0].append(p)\n",
    "            elif 'blocks.0.' in k:\n",
    "                pt_params[0].append(p)\n",
    "            elif 'blocks.1.' in k:\n",
    "                pt_params[1].append(p)\n",
    "            elif 'blocks.2.' in k:\n",
    "                pt_params[2].append(p)\n",
    "            elif 'blocks.3.' in k:\n",
    "                pt_params[3].append(p)\n",
    "            elif 'blocks.4.' in k:\n",
    "                pt_params[4].append(p)\n",
    "            elif 'blocks.5.' in k:\n",
    "                pt_params[5].append(p)\n",
    "            elif 'blocks.6.' in k:\n",
    "                pt_params[6].append(p)\n",
    "            elif 'blocks.7.' in k:\n",
    "                pt_params[7].append(p)\n",
    "            elif 'blocks.8.' in k:\n",
    "                pt_params[8].append(p)\n",
    "            elif 'blocks.9.' in k:\n",
    "                pt_params[9].append(p)\n",
    "            elif 'blocks.10.' in k:\n",
    "                pt_params[10].append(p)\n",
    "            elif 'blocks.11.' in k:\n",
    "                pt_params[11].append(p)\n",
    "            elif 'backbone.norm.weight' in k or 'backbone.norm.bias' in k:\n",
    "                pt_params[11].append(p)\n",
    "            else:\n",
    "                raise ValueError(f\"Check separate params for M2D! Unknown key: {k}\")\n",
    "        return list(reversed(pt_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664ed197-b32e-46c3-90ac-0e86432165cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Iterable, Dict, List, Optional\n",
    "\n",
    "CLASSES = ['Speech', 'Shout', 'Chainsaw', 'Jackhammer', 'Lawn Mower', 'Power Drill', 'Dog Bark', 'Rooster Crow', 'Horn Honk', 'Siren']\n",
    "\n",
    "COST_MATRIX = {\n",
    "    \"Speech\":         {\"TP\": 0,  \"FP\": 1,  \"TN\": 0, \"FN\": 5},\n",
    "    \"Dog Bark\":       {\"TP\": 0,  \"FP\": 1,  \"TN\": 0, \"FN\": 5},\n",
    "    \"Rooster Crow\":   {\"TP\": 0,  \"FP\": 1,  \"TN\": 0, \"FN\": 5},\n",
    "    \"Shout\":          {\"TP\": 0,  \"FP\": 2,  \"TN\": 0, \"FN\": 10},\n",
    "    \"Lawn Mower\":     {\"TP\": 0,  \"FP\": 3,  \"TN\": 0, \"FN\": 15},\n",
    "    \"Chainsaw\":       {\"TP\": 0,  \"FP\": 3,  \"TN\": 0, \"FN\": 15},\n",
    "    \"Jackhammer\":     {\"TP\": 0,  \"FP\": 3,  \"TN\": 0, \"FN\": 15},\n",
    "    \"Power Drill\":    {\"TP\": 0,  \"FP\": 3,  \"TN\": 0, \"FN\": 15},\n",
    "    \"Horn Honk\":      {\"TP\": 0,  \"FP\": 3,  \"TN\": 0, \"FN\": 15},\n",
    "    \"Siren\":          {\"TP\": 0,  \"FP\": 3,  \"TN\": 0, \"FN\": 15},\n",
    "}\n",
    "\n",
    "def check_dataframe(data_frame, dataset_path):\n",
    "    \"\"\"\n",
    "    Validates the integrity of a predictions or ground truth DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    predictions_df : pandas.DataFrame\n",
    "        A DataFrame containing model predictions or the ground truth.\n",
    "        It must include columns:\n",
    "        - 'filename': Name of the audio file (e.g., \"xyz.wav\")\n",
    "        - 'onset': Onset times or frame indices\n",
    "        - One column for each class in the global `CLASSES` list\n",
    "\n",
    "    dataset_path : str\n",
    "        Path to the root of the dataset directory. It must contain a\n",
    "        subdirectory 'audio_features' with `.npz` files for each audio file.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    AssertionError:\n",
    "        If any of the following checks fail:\n",
    "        - The dataset or audio_features directory doesn't exist\n",
    "        - The DataFrame is missing required columns\n",
    "        - Expected feature files are missing\n",
    "        - Number of predictions doesn't match the number of expected timesteps\n",
    "\n",
    "    Example:\n",
    "    -------\n",
    "    check_dataframe(predicted_df, \"MLPC2025_dataset\")\n",
    "    \"\"\"\n",
    "    audio_features_path = os.path.join(dataset_path, \"audio_features\")\n",
    "    assert os.path.exists(dataset_path), f\"Dataset path '{dataset_path}' does not exist.\"\n",
    "    assert os.path.exists(audio_features_path), f\"Audio features path '{audio_features_path}' does not exist.\"\n",
    "\n",
    "    required_columns = set(CLASSES + [\"filename\", \"onset\"])\n",
    "    missing_columns = required_columns - set(data_frame.columns)\n",
    "    assert not missing_columns, f\"Missing columns in predictions_df: {missing_columns}\"\n",
    "\n",
    "    assert ((data_frame[\"onset\"] / 1.2) % 1).apply(lambda x: np.isclose(x, 0, atol=0.1)).all(), \"Not all values are divisible by 1.2.\"\n",
    "    assert data_frame[CLASSES].isin([0, 1]).all().all(), \"Not all predictions are 0 or 1.\"\n",
    "\n",
    "    for filename in data_frame[\"filename\"].unique():\n",
    "        file_id = os.path.splitext(filename)[0]\n",
    "        feature_file = os.path.join(audio_features_path, f\"{file_id}.npz\")\n",
    "\n",
    "        assert os.path.exists(feature_file), f\"Feature file '{feature_file}' does not exist.\"\n",
    "\n",
    "        embeddings = np.load(feature_file)[\"embeddings\"]\n",
    "        expected_timesteps = math.ceil(len(embeddings) / 10)\n",
    "        actual_timesteps = len(data_frame[data_frame[\"filename\"] == filename])\n",
    "\n",
    "        assert actual_timesteps == expected_timesteps, (\n",
    "            f\"Mismatch in timesteps for '{filename}': expected {expected_timesteps}, found {actual_timesteps}.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def total_cost(predictions_df, ground_truth_df):\n",
    "    \"\"\"\n",
    "    Computes total cost of predictions based on a cost matrix for TP, FP, TN, and FN\n",
    "    for each class in a multilabel classification problem.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    predictions_df : pandas.DataFrame\n",
    "        DataFrame containing predicted binary labels (0 or 1) for each class in CLASSES.\n",
    "\n",
    "    ground_truth_df : pandas.DataFrame\n",
    "        DataFrame containing ground truth binary labels for each class in CLASSES.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    total_cost_value : float\n",
    "        Total cost across all classes and samples.\n",
    "\n",
    "    metrics_per_class : dict\n",
    "        Dictionary with TP, FP, TN, FN counts and cost per class.\n",
    "    \"\"\"\n",
    "\n",
    "    # Align rows by filename and onset\n",
    "    merged = predictions_df.merge(\n",
    "        ground_truth_df,\n",
    "        on=[\"filename\", \"onset\"],\n",
    "        suffixes=(\"_pred\", \"_true\"),\n",
    "        how=\"inner\",\n",
    "        validate=\"one_to_one\"\n",
    "    )\n",
    "\n",
    "    if merged.shape[0] != predictions_df.shape[0]:\n",
    "        raise ValueError(\"Mismatch in alignment between prediction and ground truth rows\")\n",
    "\n",
    "    metrics_per_class = {}\n",
    "\n",
    "    for cls in CLASSES:\n",
    "        y_pred = predictions_df[cls].astype(int)\n",
    "        y_true = ground_truth_df[cls].astype(int)\n",
    "\n",
    "        TP = ((y_pred == 1) & (y_true == 1)).mean()* 50\n",
    "        FP = ((y_pred == 1) & (y_true == 0)).mean()* 50\n",
    "        TN = ((y_pred == 0) & (y_true == 0)).mean()* 50\n",
    "        FN = ((y_pred == 0) & (y_true == 1)).mean()* 50\n",
    "\n",
    "        cost = (\n",
    "            COST_MATRIX[cls][\"TP\"] * TP +\n",
    "            COST_MATRIX[cls][\"FP\"] * FP +\n",
    "            COST_MATRIX[cls][\"TN\"] * TN +\n",
    "            COST_MATRIX[cls][\"FN\"] * FN\n",
    "        )\n",
    "\n",
    "        metrics_per_class[cls] = {\n",
    "            \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN, \"cost\": cost\n",
    "        }\n",
    "\n",
    "    return sum([metrics_per_class[c][\"cost\"] for c in metrics_per_class]), metrics_per_class\n",
    "\n",
    "\n",
    "def aggregate_targets(arr: np.ndarray, f: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Aggregates frame-level ground truths into segment-level by taking the max over fixed-size chunks.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array of shape (N, D) where N is the number of frames, D is number of classes.\n",
    "\n",
    "    f : int\n",
    "        Aggregation factor (number of frames per chunk).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Aggregated labels of shape (ceil(N/f), D)\n",
    "    \"\"\"\n",
    "    N, D = arr.shape\n",
    "    full_chunks = N // f\n",
    "    remainder = N % f\n",
    "\n",
    "    # Aggregate full chunks\n",
    "    aggregated = arr[:full_chunks * f].reshape(full_chunks, f, D).max(axis=1)\n",
    "\n",
    "    # Handle leftover frames\n",
    "    if remainder > 0:\n",
    "        tail = arr[full_chunks * f:].max(axis=0, keepdims=True)\n",
    "        aggregated = np.vstack([aggregated, tail])\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def get_ground_truth_df(filenames: Iterable[str], dataset_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and aggregates ground truth labels for an arbitrary list of files.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    filenames : Iterable[str]\n",
    "        List or array of filenames (e.g., from a subset of metadata.csv) to process.\n",
    "\n",
    "    dataset_path : str\n",
    "        Path to dataset containing the 'labels/' folder with .npz files.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: [\"filename\", \"onset\"] + CLASSES\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fname in filenames:\n",
    "        base = os.path.splitext(fname)[0]\n",
    "        label_path = os.path.join(dataset_path, 'labels', f\"{base}_labels.npz\")\n",
    "        assert os.path.exists(label_path), f\"Missing label file: {label_path}\"\n",
    "\n",
    "        y = np.load(label_path)\n",
    "        class_matrix = np.stack([y[cls].mean(-1) for cls in CLASSES], axis=1)\n",
    "        aggregated = aggregate_targets(class_matrix)\n",
    "\n",
    "        for i, row in enumerate(aggregated):\n",
    "            onset = round(i * 1.2, 1)\n",
    "            binary_labels = (row > 0).astype(int).tolist()\n",
    "            rows.append([fname, onset] + binary_labels)\n",
    "\n",
    "    return pd.DataFrame(data=rows, columns=[\"filename\", \"onset\"] + CLASSES)\n",
    "\n",
    "\n",
    "def get_segment_prediction_df(\n",
    "    predictions: Dict[str, Dict[str, np.ndarray]],\n",
    "    class_names: Optional[List[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates frame-level predictions into fixed-length segments for a set of files.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    predictions : Dict[str, Dict[str, np.ndarray]]\n",
    "        Dictionary mapping each filename to another dictionary of class-wise frame-level predictions.\n",
    "        Each class prediction is a 1D NumPy array of shape (T,), where T is time.\n",
    "\n",
    "    class_names : List[str], optional\n",
    "        List of class names to include in the output. If None, uses keys from the first file's prediction dict.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: [\"filename\", \"onset\"] + class_names.\n",
    "        Each row represents a segment and contains aggregated predictions for that segment.\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = list(next(iter(predictions.values())).keys())\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for filename, class_preds in predictions.items():\n",
    "        # Collect and stack predictions into shape (T, num_classes)\n",
    "        frame_matrix = np.stack([class_preds[cls] for cls in class_names], axis=1)\n",
    "\n",
    "        # Aggregate over fixed-length segments\n",
    "        aggregated = aggregate_targets(frame_matrix, f=10)\n",
    "\n",
    "        for seg_idx, segment in enumerate(aggregated):\n",
    "            onset = round(seg_idx * 1.2, 1)\n",
    "            rows.append([filename, onset] + segment.tolist())\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"filename\", \"onset\"] + class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cec65c6-0514-4990-924a-d10584b8604f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2DWrapper(\n",
       "  (m2d): PortableM2D(\n",
       "    (backbone): LocalViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (to_spec): MelSpectrogram(\n",
       "      Mel filter banks size = (80, 201), trainable_mel=False\n",
       "      (stft): STFT(n_fft=400, Fourier Kernel size=(201, 1, 400), iSTFT=False, trainable=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Instantiate the model\n",
    "model = M2DWrapper()\n",
    "\n",
    "# Load the original state dict\n",
    "data = torch.load(\"M2D_strong_1.pt\")\n",
    "\n",
    "# Create a new OrderedDict with replaced keys\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for k, v in data.items():\n",
    "    new_key = k.replace(\"model.\", \"m2d.\", 1)  # Replace ONLY the first occurrence\n",
    "    new_state_dict[new_key] = v\n",
    "\n",
    "# You can now save or load this into a model\n",
    "torch.save(new_state_dict, \"M2D_strong_1_m2d.pt\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb34a7b-b8d1-4ad1-8d58-877dab6cb1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio folder exists: True\n",
      "Is directory: True\n",
      "Processed 1 files.\n",
      "[(tensor([[[[-0.7138,  0.3219,  0.9397,  ..., -2.1659, -1.7354,  0.1474],\n",
      "          [ 1.3134,  0.9238,  0.3188,  ...,  1.7226, -0.7648, -0.4899],\n",
      "          [ 1.5341,  1.3754,  0.9947,  ..., -0.5720, -0.0578,  0.7657],\n",
      "          ...,\n",
      "          [-0.4178,  1.1254, -0.1166,  ..., -1.8507,  0.3180,  2.2248],\n",
      "          [ 0.4263,  0.8289,  0.2129,  ..., -0.5466, -0.6865,  0.1803],\n",
      "          [-1.0919,  0.4353,  0.2634,  ...,  0.2983, -0.1794,  0.6602]]]]), tensor([[[ 0.7724,  0.2534, -0.9733,  ..., -0.1214, -0.3854, -0.5118],\n",
      "         [-0.6734, -0.1932, -1.0854,  ...,  0.6122, -1.3088, -0.8926],\n",
      "         [-0.2172, -0.7379, -0.7474,  ..., -0.9279,  1.2254, -1.6635],\n",
      "         ...,\n",
      "         [ 0.1901,  0.3431, -2.4453,  ...,  0.6166, -0.3571, -1.9920],\n",
      "         [-1.0015, -0.5443,  0.7885,  ...,  0.5753,  0.6722,  0.4565],\n",
      "         [-1.0392,  0.3696, -1.3452,  ..., -0.5419,  1.9614,  0.1452]]]))]\n"
     ]
    }
   ],
   "source": [
    "# Example outputs\n",
    "\n",
    "model.eval()\n",
    "\n",
    "current = os.getcwd()\n",
    "audio_folder = Path(f'{current}/mlpc2025_dataset/audio')\n",
    "\n",
    "print(\"Audio folder exists:\", audio_folder.exists())\n",
    "print(\"Is directory:\", audio_folder.is_dir())\n",
    "\n",
    "outputs = []\n",
    "inputs = [torch.randn((1, 1, 80, 208))]\n",
    "for input in inputs:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "\n",
    "    outputs.append((input, output.cpu()))\n",
    "\n",
    "print(f\"Processed {len(outputs)} files.\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "868c935a-8162-4ed4-9075-bbcc3ccaa0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sample_rate = 16000\n",
    "\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=target_sample_rate,\n",
    "    n_fft=400,\n",
    "    hop_length=160,\n",
    "    n_mels=80  # <- set to 80\n",
    ")\n",
    "\n",
    "def load_and_process_audio(file_path, duration_sec=2.1):  # about 2.1 seconds\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    if sr != target_sample_rate:\n",
    "        waveform = T.Resample(sr, target_sample_rate)(waveform)\n",
    "\n",
    "    num_samples = int(target_sample_rate * duration_sec)\n",
    "    if waveform.shape[1] > num_samples:\n",
    "        waveform = waveform[:, :num_samples]\n",
    "    elif waveform.shape[1] < num_samples:\n",
    "        pad = num_samples - waveform.shape[1]\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, pad))\n",
    "\n",
    "    mel_spec = mel_transform(waveform)\n",
    "    mel_spec = torchaudio.functional.amplitude_to_DB(mel_spec, multiplier=10, amin=1e-10, db_multiplier=0)\n",
    "\n",
    "    # Ensure mel_spec has shape (1, 80, 208) — pad or crop time dimension\n",
    "    if mel_spec.shape[2] > 208:\n",
    "        mel_spec = mel_spec[:, :, :208]\n",
    "    elif mel_spec.shape[2] < 208:\n",
    "        pad_frames = 208 - mel_spec.shape[2]\n",
    "        mel_spec = torch.nn.functional.pad(mel_spec, (0, pad_frames))\n",
    "\n",
    "    mel_spec = mel_spec.unsqueeze(0)  # add batch dim\n",
    "    return mel_spec  # shape (1, 1, 80, 208)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6559d4b-a92d-4a3c-adc3-ad1cb5c381b2",
   "metadata": {},
   "source": [
    "#### NO NEED TO RUN THIS CELL MORE THEN ONCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbbd9fb-a93b-4ac1-a669-79850d7b3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def generate_output():\n",
    "outputs = []\n",
    "\n",
    "max_iter = len(list(audio_folder.glob('*.mp3')))\n",
    "audio_files = list(audio_folder.glob('*.mp3'))[:max_iter]  # take only first x files\n",
    "\n",
    "for audio_file in tqdm(audio_files, desc=\"Processing audio files\"):\n",
    "    mel_spec = load_and_process_audio(str(audio_file), duration_sec=1.2)  # 1 second audio for enough frames\n",
    "    mel_spec = mel_spec.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass expects mel spectrogram input\n",
    "        output = model(mel_spec)\n",
    "\n",
    "    outputs.append((audio_file.name, output.cpu()))\n",
    "    \n",
    "# Save\n",
    "with open('outputs.pkl', 'wb') as f:\n",
    "    pickle.dump(outputs, f)\n",
    "print(f\"Processed {len(outputs)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed63f8e-740c-4285-943d-67560f7b66f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs: 8230\n"
     ]
    }
   ],
   "source": [
    "with open('outputs.pkl', 'rb') as f:\n",
    "    outputs = pickle.load(f)\n",
    "print(f\"Number of outputs: {len(outputs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc18d869-dc4a-42c6-9eef-a76e9e73f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS ON TRAINING DATA\n",
      "\n",
      "✅ Best threshold for Speech         : 0.90 (Class cost: 3.08)\n",
      "✅ Best threshold for Shout          : 0.90 (Class cost: 1.04)\n",
      "✅ Best threshold for Chainsaw       : 0.90 (Class cost: 0.82)\n",
      "✅ Best threshold for Jackhammer     : 0.90 (Class cost: 0.55)\n",
      "✅ Best threshold for Lawn Mower     : 0.90 (Class cost: 0.78)\n",
      "✅ Best threshold for Power Drill    : 0.90 (Class cost: 1.54)\n",
      "✅ Best threshold for Dog Bark       : 0.90 (Class cost: 0.81)\n",
      "✅ Best threshold for Rooster Crow   : 0.90 (Class cost: 0.09)\n",
      "✅ Best threshold for Horn Honk      : 0.90 (Class cost: 1.95)\n",
      "✅ Best threshold for Siren          : 0.90 (Class cost: 2.08)\n",
      "\n",
      "🎯 Total Cost after Threshold Optimization: 12.75\n",
      "\n",
      "📊 Per-Class Metrics:\n",
      "Speech          → TP:   0.0, FP:   0.2, FN:   0.6, TN:   4.4, Cost:   3.08\n",
      "Shout           → TP:   0.0, FP:   0.1, FN:   0.1, TN:   5.0, Cost:   1.04\n",
      "Chainsaw        → TP:   0.0, FP:   0.0, FN:   0.1, TN:   5.2, Cost:   0.82\n",
      "Jackhammer      → TP:   0.0, FP:   0.0, FN:   0.0, TN:   5.2, Cost:   0.55\n",
      "Lawn Mower      → TP:   0.0, FP:   0.0, FN:   0.0, TN:   5.2, Cost:   0.78\n",
      "Power Drill     → TP:   0.0, FP:   0.1, FN:   0.1, TN:   5.1, Cost:   1.54\n",
      "Dog Bark        → TP:   0.0, FP:   0.0, FN:   0.2, TN:   5.1, Cost:   0.81\n",
      "Rooster Crow    → TP:   0.0, FP:   0.0, FN:   0.0, TN:   5.2, Cost:   0.09\n",
      "Horn Honk       → TP:   0.0, FP:   0.1, FN:   0.1, TN:   5.0, Cost:   1.95\n",
      "Siren           → TP:   0.0, FP:   0.1, FN:   0.1, TN:   5.0, Cost:   2.08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Convert model outputs into frame-wise probabilities\n",
    "predictions_dict = {}\n",
    "for filename, logits in outputs:\n",
    "    probs = torch.sigmoid(logits.squeeze(0))  # Shape: (T, C)\n",
    "    probs_np = probs.numpy()\n",
    "    predictions_dict[filename] = {\n",
    "        CLASSES[i]: probs_np[:, i] for i in range(len(CLASSES))\n",
    "    }\n",
    "\n",
    "# Step 2: Create segment-level prediction DataFrame\n",
    "segment_df = get_segment_prediction_df(predictions_dict)\n",
    "\n",
    "# Step 3: Load ground truth for selected filenames only\n",
    "filenames = [f.name for f in audio_files]  # From your earlier loop\n",
    "labels_path = Path(f'{current}/mlpc2025_dataset')\n",
    "ground_truth_df = get_ground_truth_df(filenames, labels_path)\n",
    "\n",
    "# Step 4: Optimize thresholds class-by-class\n",
    "best_thresholds = {}\n",
    "threshold_range = np.arange(0.1, 0.91, 0.05)\n",
    "\n",
    "print(\"LOSS ON TRAINING DATA\\n\")\n",
    "\n",
    "for cls in CLASSES:\n",
    "    min_cost = float('inf')\n",
    "    best_thresh = 0.5\n",
    "\n",
    "    for thresh in threshold_range:\n",
    "        temp_df = deepcopy(segment_df)\n",
    "\n",
    "        # Apply threshold only to the current class\n",
    "        temp_df[cls] = (temp_df[cls] > thresh).astype(int)\n",
    "        \n",
    "        # Keep other classes binarized at 0.5\n",
    "        for other_cls in CLASSES:\n",
    "            if other_cls != cls:\n",
    "                temp_df[other_cls] = (temp_df[other_cls] > 0.5).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        _, class_metrics = total_cost(temp_df, ground_truth_df)\n",
    "\n",
    "        # Use only this class's cost for optimization\n",
    "        cost = class_metrics[cls]['cost']\n",
    "\n",
    "        if cost < min_cost:\n",
    "            min_cost = cost\n",
    "            best_thresh = thresh\n",
    "\n",
    "    best_thresholds[cls] = best_thresh\n",
    "    print(f\"✅ Best threshold for {cls:15}: {best_thresh:.2f} (Class cost: {min_cost:.2f})\")\n",
    "\n",
    "# Step 5: Apply best thresholds to all classes\n",
    "final_df = deepcopy(segment_df)\n",
    "for cls in CLASSES:\n",
    "    final_df[cls] = (final_df[cls] > best_thresholds[cls]).astype(int)\n",
    "\n",
    "# Step 6: Compute final cost and detailed metrics\n",
    "total, class_metrics = total_cost(final_df, ground_truth_df)\n",
    "\n",
    "# Step 7: Display results\n",
    "print(f\"\\n🎯 Total Cost after Threshold Optimization: {total:.2f}\")\n",
    "print(\"\\n📊 Per-Class Metrics:\")\n",
    "\n",
    "for cls, metrics in class_metrics.items():\n",
    "    # Round metrics for interpretability\n",
    "    TP = round(metrics['TP'], 1)\n",
    "    FP = round(metrics['FP'], 1)\n",
    "    FN = round(metrics['FN'], 1)\n",
    "    TN = round(metrics.get('TN', 0), 1)  # Optional TN\n",
    "    cost = round(metrics['cost'], 2)\n",
    "\n",
    "    print(f\"{cls:15} → TP: {TP:>5}, FP: {FP:>5}, FN: {FN:>5}, TN: {TN:>5}, Cost: {cost:6.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886fe0aa-7417-48dd-8730-2c29f2d609cf",
   "metadata": {},
   "source": [
    "#### NO NEED TO RUN THIS CELL MORE THEN ONCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3525b8a9-cf30-4f23-860d-148392cd9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def generate_output():\n",
    "outputs_test = []\n",
    "\n",
    "current = os.getcwd()\n",
    "audio_folder = Path(f'{current}/mlpc2025_test/audio')\n",
    "\n",
    "max_iter = len(list(audio_folder.glob('*.mp3')))\n",
    "audio_files = list(audio_folder.glob('*.mp3'))[:max_iter]  # take only first x files\n",
    "\n",
    "for audio_file in tqdm(audio_files, desc=\"Processing audio files\"):\n",
    "    mel_spec = load_and_process_audio(str(audio_file), duration_sec=1.2)  # 1 second audio for enough frames\n",
    "    mel_spec = mel_spec.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass expects mel spectrogram input\n",
    "        output = model(mel_spec)\n",
    "\n",
    "    outputs_test.append((audio_file.name, output.cpu()))\n",
    "    \n",
    "# Save\n",
    "with open('outputs_test.pkl', 'wb') as f:\n",
    "    pickle.dump(outputs_test, f)\n",
    "print(f\"Processed {len(outputs_test)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "861515cc-4d88-45c6-b5b1-b8a5063a60e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outputs: 2742\n"
     ]
    }
   ],
   "source": [
    "with open('outputs_test.pkl', 'rb') as f:\n",
    "    outputs_test = pickle.load(f)\n",
    "print(f\"Number of outputs: {len(outputs_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "766daab9-04e6-4a69-8fef-e1a268dcc2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS ON TEST DATA\n",
      "\n",
      "✅ Best threshold for Speech         : 0.90 (Class cost: 0.10)\n",
      "✅ Best threshold for Shout          : 0.90 (Class cost: 0.04)\n",
      "✅ Best threshold for Chainsaw       : 0.90 (Class cost: 0.05)\n",
      "✅ Best threshold for Jackhammer     : 0.90 (Class cost: 0.14)\n",
      "✅ Best threshold for Lawn Mower     : 0.90 (Class cost: 0.05)\n",
      "✅ Best threshold for Power Drill    : 0.90 (Class cost: 0.14)\n",
      "✅ Best threshold for Dog Bark       : 0.90 (Class cost: 0.05)\n",
      "✅ Best threshold for Rooster Crow   : 0.90 (Class cost: 0.01)\n",
      "✅ Best threshold for Horn Honk      : 0.90 (Class cost: 0.14)\n",
      "✅ Best threshold for Siren          : 0.90 (Class cost: 0.23)\n",
      "\n",
      "🎯 Total Cost after Threshold Optimization: 0.94\n",
      "\n",
      "📊 Per-Class Metrics:\n",
      "Speech          → TP:   0.0, FP:   0.1, FN:   0.0, TN:   5.2, Cost:   0.10\n",
      "Shout           → TP:   0.0, FP:   0.0, FN:   0.0, TN:   5.2, Cost:   0.04\n",
      "Chainsaw        → TP:   0.0, FP:   0.0, FN:   0.0, TN:   5.2, Cost:   0.05\n",
      "Jackhammer      → TP:   0.0, FP:   0.0, FN:   0.0, TN:   5.2, Cost:   0.14\n",
      "Lawn Mower      → TP:   0.0, FP:   0.0, FN:   0.0, TN:   5.2, Cost:   0.05\n",
      "Power Drill     → TP:   0.0, FP:   0.0, FN:   0.0, TN:   5.2, Cost:   0.14\n",
      "Dog Bark        → TP:   0.0, FP:   0.1, FN:   0.0, TN:   5.2, Cost:   0.05\n",
      "Rooster Crow    → TP:   0.0, FP:   0.0, FN:   0.0, TN:   5.2, Cost:   0.01\n",
      "Horn Honk       → TP:   0.0, FP:   0.0, FN:   0.0, TN:   5.2, Cost:   0.14\n",
      "Siren           → TP:   0.0, FP:   0.1, FN:   0.0, TN:   5.2, Cost:   0.23\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert model outputs into frame-wise probabilities\n",
    "predictions_dict_test = {}\n",
    "for filename, logits in outputs_test:\n",
    "    probs = torch.sigmoid(logits.squeeze(0))  # Shape: (T, C)\n",
    "    probs_np = probs.numpy()\n",
    "    predictions_dict_test[filename] = {\n",
    "        CLASSES[i]: probs_np[:, i] for i in range(len(CLASSES))\n",
    "    }\n",
    "\n",
    "# Step 2: Create segment-level prediction DataFrame\n",
    "segment_df_test = get_segment_prediction_df(predictions_dict_test)\n",
    "\n",
    "# Step 3: Load ground truth for selected filenames only\n",
    "csv_path = f'{current}/mlpc2025_test/ground_truth.csv'\n",
    "ground_truth_df_test = pd.read_csv(csv_path)\n",
    "\n",
    "# Step 4: Optimize thresholds class-by-class\n",
    "best_thresholds_test = {}\n",
    "threshold_range_test = np.arange(0.1, 0.91, 0.05)\n",
    "\n",
    "print(\"LOSS ON TEST DATA\\n\")\n",
    "\n",
    "for cls in CLASSES:\n",
    "    min_cost = float('inf')\n",
    "    best_thresh = 0.5\n",
    "\n",
    "    for thresh in threshold_range_test:\n",
    "        temp_df = deepcopy(segment_df_test)\n",
    "\n",
    "        # Apply threshold only to the current class\n",
    "        temp_df[cls] = (temp_df[cls] > thresh).astype(int)\n",
    "        \n",
    "        # Keep other classes binarized at 0.5\n",
    "        for other_cls in CLASSES:\n",
    "            if other_cls != cls:\n",
    "                temp_df[other_cls] = (temp_df[other_cls] > 0.5).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        _, class_metrics = total_cost(temp_df, ground_truth_df_test)\n",
    "\n",
    "        # Use only this class's cost for optimization\n",
    "        cost = class_metrics[cls]['cost']\n",
    "\n",
    "        if cost < min_cost:\n",
    "            min_cost = cost\n",
    "            best_thresh = thresh\n",
    "\n",
    "    best_thresholds_test[cls] = best_thresh\n",
    "    print(f\"✅ Best threshold for {cls:15}: {best_thresh:.2f} (Class cost: {min_cost:.2f})\")\n",
    "\n",
    "# Step 5: Apply best thresholds to all classes\n",
    "final_df_test = deepcopy(segment_df_test)\n",
    "for cls in CLASSES:\n",
    "    final_df_test[cls] = (final_df_test[cls] > best_thresholds_test[cls]).astype(int)\n",
    "\n",
    "# Step 6: Compute final cost and detailed metrics\n",
    "total, class_metrics = total_cost(final_df_test, ground_truth_df_test)\n",
    "\n",
    "# Step 7: Display results\n",
    "print(f\"\\n🎯 Total Cost after Threshold Optimization: {total:.2f}\")\n",
    "print(\"\\n📊 Per-Class Metrics:\")\n",
    "\n",
    "for cls, metrics in class_metrics.items():\n",
    "    # Round metrics for interpretability\n",
    "    TP = round(metrics['TP'], 1)\n",
    "    FP = round(metrics['FP'], 1)\n",
    "    FN = round(metrics['FN'], 1)\n",
    "    TN = round(metrics.get('TN', 0), 1)  # Optional TN\n",
    "    cost = round(metrics['cost'], 2)\n",
    "\n",
    "    print(f\"{cls:15} → TP: {TP:>5}, FP: {FP:>5}, FN: {FN:>5}, TN: {TN:>5}, Cost: {cost:6.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec459533-097e-4233-b387-788b797d2536",
   "metadata": {},
   "source": [
    "### No training required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d691d5-be82-4caa-aa7a-31591b5f5ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
